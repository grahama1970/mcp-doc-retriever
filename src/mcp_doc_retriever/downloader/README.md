# MCP Document Retriever Service ğŸŒğŸ’¾ğŸ”

## Overview ğŸŒŸ

`mcp-doc-retriever` is a Dockerized FastAPI application designed to act as a Model Context Protocol (MCP) server, primarily for AI agents. Its core function is to download documentation content from various sources (Git repositories, websites via HTTPX/Playwright), store it locally, and provide API endpoints to manage downloads and search the retrieved content.

The service initiates downloads asynchronously, allowing agents to start a job and poll for its completion status using a unique `download_id`. It uses efficient `httpx` requests by default for web crawling but supports `playwright` for JavaScript-heavy pages. Git repositories can be cloned fully or using sparse checkout. Downloads are stored locally within a persistent volume. Git content retains its structure, while website content is stored using a secure, flat structure (`<hostname>/<filename>-<hash>.<ext>`) to prevent path traversal issues and ensure uniqueness. A detailed index file is created for each download job, tracking URLs/files, local paths, and fetch statuses. The search functionality enables agents to first quickly scan relevant files for keywords and then perform precise text extraction using CSS selectors or analyse structured content blocks (code/JSON).

This project is intended to be built and potentially maintained using an agentic workflow, specifically following the Roomodes framework described below.

## âœ¨ Features

*   âœ… **Multi-Source Download:** Supports 'git', 'website' (HTTPX), and 'playwright' source types. *(Handled by `downloader` package)*
*   âœ… **Recursive Website Crawling:** Downloads HTML content starting from a URL, following links within the same domain (configurable depth). *(Handled by `downloader.web`)*
*   âœ… **Git Repository Cloning:** Clones Git repos, supports sparse checkout via `doc_path`. *(Handled by `downloader.git`)*
*   âœ… **Secure Local Storage:**
    *   Git: Saves cloned repo preserving Git structure within `<download_id>/repo/`.
    *   Website/Playwright: Saves downloaded files using a flat, secure structure `<download_id>/<sanitized_hostname>/<sanitized_url_base>-<hash>.<ext>` to ensure uniqueness and prevent path traversal exploits inherent in mirroring URL paths directly to the filesystem. *(Handled by `downloader.helpers.url_to_local_path`)*
*   âœ… **Asynchronous Downloads & Status Polling:** Downloads run in the background via FastAPI `BackgroundTasks`. A dedicated `/status/{download_id}` endpoint allows agents to poll for task completion. *(Handled by `main.py`)*
*   âœ… **Download Indexing:** Maintains a JSON Lines index file per download job (`<download_id>.jsonl`), mapping original URLs/files to canonical URLs/paths, the generated local file path, content MD5 hashes, and detailed fetch status. *(Generated by `downloader.web`, used by `searcher`)*
*   âœ… **Efficient Re-fetching/Cloning:** Avoids re-downloading/cloning if content exists unless overridden by `force=true`. *(Handled by `downloader` package)*
*   âœ… **Robots.txt Respect:** Checks and adheres to `robots.txt` rules for website crawling. *(Handled by `downloader.robots`)*
*   âœ… **Two-Phase Search (Job-Scoped):** *(Handled by `searcher` package)*
    1.  **Fast Scan:** Uses the index file to identify relevant local files for a specific `download_id`, then quickly scans the decoded text content for keywords. *(searcher.scanner)*
    2.  **Precise Extraction:** Parses candidate pages (identified by scan) using BeautifulSoup and applies CSS selectors to extract specific text content. Can further filter results by keywords. *(searcher.basic_extractor)*
*   âœ… **Advanced Content Block Extraction:** Can parse HTML/Markdown into structured blocks (text, code, JSON) for more targeted analysis. *(Handled by `searcher.advanced_extractor` and `searcher.helpers`)*
*   âœ… **Concurrency Control:** Uses `asyncio` Semaphores (web) and `ThreadPoolExecutor` (git/sync tasks).
*   âœ… **Structured I/O:** Uses Pydantic models for robust API request/response validation. *(models.py)*
*   âœ… **Dockerized & Self-Contained:** Packaged with `docker compose`, includes Playwright browser dependencies, uses a named volume for persistence.
*   âœ… **Configuration:** Supports configuration via environment variables or `config.json`. *(config.py)*
*   âœ… **Standard Packaging:** Uses `pyproject.toml` and `uv`.
*   âœ… **Modular Structure:** Code organized into `downloader` and `searcher` sub-packages.

## ğŸ—ï¸ Runtime Architecture Diagram

*(The Mermaid diagram code itself remains the same, but the text description below should reflect the new module paths where relevant)*

```mermaid
graph TD
    subgraph "External Agent (e.g., Roo Code)"
        Agent -- "1. POST /download\n(DocDownloadRequest JSON)" --> FAPI
        AgentResp1 -- "2. TaskStatus (pending, download_id)" --> Agent
        Agent -- "3. GET /status/{download_id}\n(Repeat until completed/failed)" --> FAPI
        AgentResp2 -- "4. TaskStatus JSON" --> Agent
        Agent -- "5. POST /search\n(SearchRequest JSON - If completed)" --> FAPI
        AgentResp3 -- "6. List[SearchResultItem] JSON" --> Agent
    end

    subgraph "Docker Container: mcp-doc-retriever"
        FAPI("ğŸŒ FastAPI - main.py") -- Manages --> TaskStore("ğŸ“ Task Status Store (In-Memory)")
        FAPI -- Uses --> SharedExecutor("ğŸ”„ ThreadPoolExecutor")

        subgraph "Download Flow (Background Task)"
            direction TB
            FAPI -- Trigger --> BGTaskWrapper("ğŸš€ Run Download Wrapper")
            BGTaskWrapper -- Update Status (running) --> TaskStore
            BGTaskWrapper -- "Params + Executor" --> Workflow("âš™ï¸ Downloader Workflow - downloader/workflow.py")

            subgraph "Git Source"
                Workflow -- "Clone/Scan" --> GitDownloader("ğŸ™ Git Downloader - downloader/git.py")
                GitDownloader -- "git cmd" --> SharedExecutor
                GitDownloader -- "File Scan" --> SharedExecutor
                GitDownloader -- "Files List" --> Workflow
            end

            subgraph "Web Source (Website/Playwright)"
                Workflow -- "Crawl" --> WebDownloader("ğŸ•¸ï¸ Web Downloader - downloader/web.py")
                WebDownloader -- "Check Robots" --> RobotsUtil("ğŸ¤– Robots Util - downloader/robots.py")
                # WebDownloader calls url_to_local_path from helpers
                WebDownloader -- "Calc Path" --> HelperFuncs("ğŸ› ï¸ Helpers - downloader/helpers.py")
                WebDownloader -- "Fetch URL" --> Fetchers("â¬‡ï¸ Fetchers - downloader/fetchers.py")
                Fetchers -- "HTTP" --> HttpxLib("ğŸ httpx")
                Fetchers -- "Browser" --> PlaywrightLib("ğŸ­ Playwright")
                HttpxLib --> TargetSite("ğŸŒ Target Website/Repo")
                PlaywrightLib --> TargetSite
                TargetSite -- "Content" --> Fetchers
                Fetchers -- "Result" --> WebDownloader
                # Fetchers save content based on path from WebDownloader
                WebDownloader -- "Save Content + Log Index" --> Storage{{ğŸ’¾ Storage Volume}}
                WebDownloader -- "Extract Links" --> WebDownloader("Recursive Call")
            end

            Workflow -- "Result/Error" --> BGTaskWrapper
            BGTaskWrapper -- "Update Status (completed/failed)" --> TaskStore
        end

        subgraph "Search Flow"
            direction TB
            FAPI -- "Parse SearchRequest" --> SearcherCore("ğŸ” Searcher Core - searcher/searcher.py")
            SearcherCore -- "Read Index" --> Storage
            Storage -- "File Paths + URLs" --> SearcherCore
            SearcherCore -- "Paths + Keywords" --> Scanner("ğŸ“° Keyword Scanner - searcher/scanner.py")
            Scanner -- "Read Files" --> Storage
            Scanner -- "Candidate Paths" --> SearcherCore
            SearcherCore -- "Paths + Selector" --> BasicExtractor("âœ‚ï¸ Basic Extractor - searcher/basic_extractor.py")
            BasicExtractor -- "Read Files" --> Storage
            BasicExtractor -- "Snippets" --> SearcherCore
            SearcherCore -- "Format Results" --> FAPI
        end

        subgraph "Storage Volume (download_data)"
            direction TB
            Storage -- Contains --> IndexDir("index/*.jsonl")
            # Clarify content storage structure
            Storage -- Contains --> ContentDir("content/<download_id>/repo/ (Git)")
            Storage -- Contains --> ContentDirWeb("content/<download_id>/<host>/<fname>-<hash>.<ext> (Web)")
        end
    end
```

*Diagram Key:* The diagram shows the agent interaction flow (1-6), the background task execution for downloads, and the synchronous flow for search. Components are labeled with their corresponding module file paths. The shared executor and storage volume (clarifying web storage structure) are highlighted.

## ğŸ› ï¸ Technology Stack

*   **Web Framework:** FastAPI
*   **CLI Framework:** Typer
*   **HTTP Client:** HTTPX (for async requests)
*   **Web Browser Automation:** Playwright (optional, for JS-heavy sites)
*   **Data Validation:** Pydantic
*   **Async File I/O:** aiofiles
*   **Filesystem Path Handling:** pathlib
*   **Filename/Path Sanitization:** pathvalidate
*   **HTML/XML Parsing:** BeautifulSoup4 (`bs4`)
*   **Markdown Parsing:** CommonMark (`commonmark`)
*   **Progress Bars:** tqdm
*   **Concurrency:** asyncio, concurrent.futures (ThreadPoolExecutor)
*   **Containerization:** Docker, Docker Compose
*   **Dependency Management:** uv

## ğŸ¤– Roomodes Workflow (Project Construction)

This project is designed to be developed and maintained using an agentic workflow based on the "Roomodes" concept. This involves different AI agent "modes" with specific roles and capabilities:

*   **Planner:** Defines high-level goals, breaks them down into tasks (`task.md`), manages overall progress, and assigns tasks to Boomerang Mode.
*   **Boomerang Mode:** Acts as a middle manager. Receives tasks from the Planner, delegates specific coding/debugging/research steps to specialized coder agents, aggregates results, and reports back to the Planner. Handles escalations.
*   **Coder Agents (Intern, Junior, Senior):** Handle coding tasks of varying complexity. Follow specific protocols defined in `.roorules`. The Senior Coder handles complex logic, architecture, and escalated issues.
*   **Researcher:** Finds relevant documentation, libraries, or solutions using external search tools (like Perplexity).
*   **Hacker:** Focuses on security analysis, identifying vulnerabilities, and suggesting fixes.
*   **Presenter:** Formats final outputs, updates documentation (like this README), and prepares results.

The `.roomodes` file defines the specific prompts and capabilities for each agent mode, while `.roorules` defines global coding standards, error handling procedures, and verification requirements applicable to all relevant agents.

## ğŸ“ Project Structure (Refactored)

*Note: File paths below reflect the new structure.*
```
mcp-doc-retriever/
â”œâ”€â”€ .git/
â”œâ”€â”€ .gitignore
â”œâ”€â”€ .env.example        # Example environment variables
â”œâ”€â”€ .venv/              # Virtual environment (if used locally)
â”œâ”€â”€ .roomodes           # Agent mode definitions
â”œâ”€â”€ .roorules           # Global rules governing agent behavior
â”œâ”€â”€ docker-compose.yml  # Docker Compose service definition
â”œâ”€â”€ Dockerfile          # Docker image build instructions
â”œâ”€â”€ pyproject.toml      # Project metadata and dependencies (for uv/pip)
â”œâ”€â”€ uv.lock             # Pinned dependency versions
â”œâ”€â”€ README.md           # This file
â”œâ”€â”€ task.md             # High-level task plan for Planner agent
â”œâ”€â”€ config.json         # Optional local config file (overridden by env vars)
â”œâ”€â”€ repo_docs/          # Downloaded third-party documentation for agent reference
â”‚   â””â”€â”€ ...
â”œâ”€â”€ scripts/
â”‚   â””â”€â”€ test_runner.sh  # End-to-end sanity check script
â””â”€â”€ src/
    â””â”€â”€ mcp_doc_retriever/ # Main application source code
        â”œâ”€â”€ __init__.py       # Make src/mcp_doc_retriever a package
        â”œâ”€â”€ cli.py            # Main CLI entry point (Typer app)
        â”œâ”€â”€ config.py         # Configuration loading
        â”œâ”€â”€ main.py           # FastAPI app, API endpoints, status store
        â”œâ”€â”€ models.py         # Pydantic models (API, Index, Status, ContentBlock)
        â”œâ”€â”€ utils.py          # Shared utilities (URL canonicalization, SSRF check, keyword matching)
        â”œâ”€â”€ docs/             # Internal project documentation / lessons
        â”‚   # (lessons_learned.json is obsolete, replaced by ArangoDB)
        â”œâ”€â”€ downloader/       # --- Sub-package for Downloading ---
        â”‚   â”œâ”€â”€ __init__.py   # Make downloader a package
        â”‚   â”œâ”€â”€ workflow.py   # Main download orchestration logic
        â”‚   â”œâ”€â”€ git_downloader.py # Git clone/scan logic (renamed from git.py)
        â”‚   â”œâ”€â”€ web_downloader.py # Web crawling logic (renamed from web.py)
        â”‚   â”œâ”€â”€ fetchers.py   # HTTPX and Playwright fetch implementations
        â”‚   â”œâ”€â”€ robots.py     # robots.txt parsing logic
        â”‚   â””â”€â”€ helpers.py    # Downloader-specific helpers (e.g., url_to_local_path)
        â””â”€â”€ searcher/         # --- Sub-package for Searching ---
            â”œâ”€â”€ __init__.py   # Make searcher a package
            â”œâ”€â”€ searcher.py     # Basic search orchestration (perform_search)
            â”œâ”€â”€ scanner.py      # Keyword scanning logic
            â”œâ”€â”€ basic_extractor.py # Basic text snippet extraction
            â”œâ”€â”€ advanced_extractor.py # Advanced block-based extraction
            â””â”€â”€ helpers.py    # Search-specific helpers (file access, content parsing)

# Download data lives in the Docker volume 'download_data', mapped to /app/downloads.
# /app/downloads/index/ contains *.jsonl index files
# /app/downloads/content/<download_id>/ contains downloaded files/repo clones
```

## âš™ï¸ Configuration

Configuration is managed via `src/mcp_doc_retriever/config.py`. Values can be set through:

1.  **Environment Variables:** (Highest priority) Prefixed with `MCP_DOC_`. e.g., `MCP_DOC_BASE_DIR=/data/downloads`. See `.env.example`.
2.  **`config.json` file:** A JSON file in the project root.
3.  **Default Values:** Defined in `config.py`.

Key configurable values include:
*   `BASE_DIR`: Root directory for downloads (`/app/downloads` inside Docker).
*   `DEFAULT_DEPTH`: Default crawl depth.
*   `TIMEOUT_REQUESTS`: Default timeout for HTTPX requests.
*   `TIMEOUT_PLAYWRIGHT`: Default timeout for Playwright operations.
*   `MAX_FILE_SIZE`: Default max file size limit.
*   `MAX_CONCURRENT_REQUESTS`: Default concurrency limit for web downloads.

## ğŸ› ï¸ MCP Server Configuration Example

*(This section remains the same)*

## ğŸ› ï¸ Setup & Installation

1.  **Prerequisites:**
    *   Docker & Docker Compose ([Install Docker](https://docs.docker.com/engine/install/))
    *   Git (for cloning this repo and the `git` downloader source type)
    *   `uv` (recommended, for local development environment) - ([Install uv](https://github.com/astral-sh/uv))
2.  **Clone the repository:**
    ```bash
    git clone <repository_url>
    cd mcp-doc-retriever
    ```
3.  **(Optional) Local Development Setup:**
    ```bash
    # Create a virtual environment (uv recommended)
    uv venv .venv
    source .venv/bin/activate
    # Install dependencies including dev tools
    uv pip install -e .[dev]
    # Install Playwright browsers (needed for playwright source type)
    playwright install --with-deps
    ```
4.  **(Optional) Configure Environment:** Copy `.env.example` to `.env` and adjust variables if needed.

## ğŸš€ Running the Service

The primary way to run the service is using Docker Compose:

```bash
# Build and start the services in detached mode
docker compose up --build -d
```

This will:
*   Build the Docker image based on `Dockerfile`.
*   Start the `mcp-doc-retriever` FastAPI service container.
*   Start supporting services if defined (e.g., a database - currently none).
*   Create/use the named volume `download_data` for persistent storage.

The API will typically be available at `http://localhost:8001` (or the port configured in `docker-compose.yml`).

To view logs:
```bash
docker compose logs -f mcp-doc-retriever
```

To stop the services:
```bash
docker compose down
```

## ğŸ’» API Usage

Interact with the running service via its HTTP endpoints:

*   **`POST /download`**: Start a new download job.
    *   **Request Body:** `DocDownloadRequest` JSON (see `models.py`) - includes `source_type`, `download_id`, source locations (`url` or `repo_url`/`doc_path`), and options (`depth`, `force`, etc.).
    *   **Response:** `TaskStatus` JSON indicating initial status (`pending` or `failed_validation`) and the `download_id`.
*   **`GET /status/{download_id}`**: Check the status of a download job.
    *   **Response:** `TaskStatus` JSON with current status (`pending`, `running`, `completed`, `failed`), progress (if available), start/end times, and error message if failed.
*   **`POST /search`**: Perform a search on a *completed* download job.
    *   **Request Body:** `SearchRequest` JSON (see `models.py`) - includes `download_id`, `keywords`, optional `css_selector`, and filtering options.
    *   **Response:** `List[SearchResultItem]` JSON containing found snippets/blocks matching the criteria.

*(Refer to `main.py` for exact endpoint definitions and `models.py` for request/response structures.)*

## ğŸ¤” Key Concepts Explained

*   **`download_id`:** A user-provided unique identifier for a specific download task. All downloaded content and the index file for that task are stored relative to this ID.
*   **`TaskStatus`:** Represents the state of a download job (pending, running, completed, failed). Polled via the `/status` endpoint.
*   **`IndexRecord`:** A JSON object (stored line-by-line in `.jsonl` files) containing metadata for each downloaded URL/file (canonical URL, local path, MD5, status, etc.). Crucial for linking URLs to local content and enabling search.
*   **Canonical URL:** A normalized version of a URL (e.g., scheme lowercased, default ports removed, path simplified) used for consistent identification and tracking in the visited set and index. *(Handled by `utils.canonicalize_url`)*
*   **Flat Web Storage:** To prevent security risks and complexity associated with mapping arbitrary URL paths directly to filesystem paths, web content is stored flatly within a directory named after the sanitized hostname. Filenames are generated from the sanitized URL string + a hash for uniqueness.

## ğŸ§ª Testing

*   **Standalone Module Tests:** Each core module includes an `if __name__ == "__main__":` block for basic, independent execution and verification (`uv run <filename>`).
*   **Unit Tests:** (Located in `tests/unit/`) Use `pytest` to test individual functions in isolation. Focus on helpers and utilities.
*   **Integration Tests:** (Located in `tests/integration/`) Use `pytest` and `pytest-asyncio` to test interactions between components (e.g., full download and index creation flow for web, git clone and scan). May use external services like `httpbin.org` or local mocks.
*   **End-to-End Tests:** (`scripts/test_runner.sh`) A bash script that interacts with the *running Docker container* via `curl`, simulating agent API calls for download, status polling, and search to verify the complete system flow.

Run tests locally (after setting up the dev environment):
```bash
# Run all tests
uv run pytest

# Run specific tests
uv run pytest tests/unit/downloader/test_helpers.py
```

## ğŸ“š Documentation Standards

This project adheres to specific documentation standards, primarily governed by the `.roorules` file:

*   **Module Docstrings:** Every core `.py` file within `src/mcp_doc_retriever/` **must** include a module-level docstring at the top containing:
    *   A clear **description** of the module's purpose.
    *   Direct **links** to the official documentation for any third-party packages used within that module.
    *   A concrete **sample input** and the corresponding **expected output** for the module's primary function(s).
*   **Standalone Verification Block:** Every core `.py` file **must** include a functional `if __name__ == "__main__":` block. This block should contain code that provides a **minimal, real-world usage example** demonstrating and verifying the core functionality of that specific module *independently*, without relying on the full FastAPI server or Docker environment.
*   **Code Comments:** Use inline comments (`#`) to explain complex algorithms, business logic decisions, assumptions, or potential workarounds that aren't immediately obvious from the code itself.
*   **README Accuracy:** This `README.md` file should be kept up-to-date with the project's features, API specifications, setup instructions, and core concepts.
*   **Agent Knowledge Base:**
    *   **Lessons Learned:** Reusable solutions, non-obvious fixes, or valuable insights discovered during development (especially by agents) are logged in the `lessons_learned` collection in the ArangoDB database (see main README and `.roorules`).
    *   **Repository Docs:** Relevant documentation for third-party libraries used in the project should be stored in the `repo_docs/` directory for agent reference.
```