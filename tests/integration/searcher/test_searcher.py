# File: tests/integration/searcher/test_searcher.py

"""
Module: test_searcher.py

Description:
Integration tests for the searcher component (`searcher.py`, `scanner.py`,
`basic_extractor.py`). Verifies search orchestration using test data that reflects
the final flat/hashed path structure.
"""

# --- Module Header ---
# Description:
#   This module contains integration tests for the search functionality defined in
#   `src/mcp_doc_retriever/searcher/searcher.py`. It verifies that the
#   `perform_search` function correctly orchestrates the following steps:
#   1. Reading the download index file (`.jsonl`).
#   2. Identifying relevant local content files based on the index and keywords.
#   3. Scanning the content of those files for keywords.
#   4. Optionally extracting text snippets using CSS selectors via BeautifulSoup.
#   5. Filtering results based on keywords within extracted snippets.
#   6. Returning the results as a list of `SearchResultItem` objects.
#
#   Crucially, these tests use a fixture (`search_test_environment`) that creates mock
#   downloaded content and index files mirroring the **flat, hashed path structure**
#   generated by the `downloader.helpers.original_url_to_local_path` function for web downloads.
#   This ensures the search component works correctly with the actual storage format.
#
# Third-Party Documentation:
#   - pytest: https://docs.pytest.org/
#
# Internal Module Dependencies:
#   - mcp_doc_retriever.searcher.searcher (perform_search)
#   - mcp_doc_retriever.models (SearchResultItem, IndexRecord, SearchRequest)
#   - mcp_doc_retriever.downloader.helpers (url_to_local_path - used by fixture)
#   - mcp_doc_retriever.utils (canonicalize_url - used by fixture)
#
# Sample Input (Conceptual - Test Function Call):
#   query = SearchRequest(download_id="test_search_id", scan_keywords=["important"], extract_selector=".highlight")
#   results = searcher.perform_search(query, base_download_dir)
#
# Sample Expected Output (Conceptual - Test Assertion):
#   assert len(results) == 2
#   assert results[0].original_url == "http://example.com/section/unique_page"
#   assert "Important APPLE Info" in results[0].match_details
# --- End Module Header ---

import pytest
import json
import hashlib
import shutil
from pathlib import Path
from datetime import datetime, timezone  # Use timezone aware datetime

# --- Core Imports ---
from mcp_doc_retriever.searcher import searcher
from mcp_doc_retriever.models import SearchResultItem, IndexRecord, SearchRequest

# --- Helper Imports (Needed for Test Setup ONLY) ---
from mcp_doc_retriever.downloader.helpers import url_to_local_path

try:
    from mcp_doc_retriever.utils import canonicalize_url
except ImportError:

    def canonicalize_url(url: str) -> str:
        return url


# --- Test Data ---
URL_PAGE1 = "http://example.com/page1.html"
URL_PAGE2 = "http://example.com/section/unique_page"
URL_PAGE3 = "http://anotherexample.org/different/page"
URL_CSS = "http://example.com/style.css"

CONTENT1 = """
<!DOCTYPE html><html><head><title>Page 1</title></head><body>
<h1>Welcome</h1><p>This is the first test page with the keyword APPLE.</p>
<div class="data">Data point 1</div>
<a href="/section/unique_page">Link to Page 2</a>
</body></html>
"""
CONTENT2 = """
<!DOCTYPE html><html><head><title>Page 2</title></head><body>
<h2>Second Page (Unique)</h2><p>Content for the second page, including the keyword BANANA.</p>
<div class="data">Data point 2</div>
<span class="highlight">Important APPLE Info</span>
</body></html>
"""
CONTENT3 = """
<!DOCTYPE html><html><head><title>Page 3</title></head><body>
<h1>Another Site</h1><p>Third page content, keyword ORANGE.</p>
<div class="other-data">Data point 3</div>
<span class="highlight">More ORANGE Info</span>
</body></html>
"""
CONTENT_CSS = "body { color: red; /* Keyword: APPLE */ }"


# --- Fixtures ---
@pytest.fixture(scope="function")
def search_test_environment(tmp_path: Path):
    """
    Sets up a temporary directory structure mimicking downloads/index and
    downloads/content, using the actual `url_to_local_path` helper.
    """
    base_dir = tmp_path / "downloads_search"
    download_id = "search_test_fixture"
    index_dir = base_dir / "index"
    content_base_dir = base_dir / "content" / download_id

    index_dir.mkdir(parents=True, exist_ok=True)

    test_data = [
        {"url": URL_PAGE1, "content": CONTENT1, "content_type": "text/html"},
        {"url": URL_PAGE2, "content": CONTENT2, "content_type": "text/html"},
        {"url": URL_PAGE3, "content": CONTENT3, "content_type": "text/html"},
        {"url": URL_CSS, "content": CONTENT_CSS, "content_type": "text/css"},
    ]

    index_file_path = index_dir / f"{download_id}.jsonl"
    index_records_to_write = []
    created_files_map = {}

    for item_data in test_data:
        url = item_data["url"]
        content = item_data["content"]
        content_type = item_data["content_type"]

        try:
            absolute_target_path = url_to_local_path(content_base_dir, url)
            absolute_target_path.parent.mkdir(parents=True, exist_ok=True)
            absolute_target_path.write_text(content, encoding="utf-8")
            created_files_map[url] = absolute_target_path

            record = IndexRecord(
                original_url=url,
                canonical_url=canonicalize_url(url),
                local_path=str(absolute_target_path),
                fetch_status="success",
                http_status=200,
                content_type=content_type,
                timestamp=datetime.now(timezone.utc).isoformat(),
                content_md5=hashlib.md5(content.encode()).hexdigest(),
            )
            index_records_to_write.append(record)
        except Exception as e:
            pytest.fail(f"Fixture setup failed creating file/record for {url}: {e}")

    try:
        with open(index_file_path, "w", encoding="utf-8") as f:
            for record in index_records_to_write:
                f.write(record.model_dump_json(exclude_none=True) + "\n")
    except Exception as e:
        pytest.fail(f"Fixture setup failed writing index file {index_file_path}: {e}")

    yield {
        "base_dir": base_dir,
        "index_file": index_file_path,
        "content_base_dir": content_base_dir,
        "download_id": download_id,
        "created_files": created_files_map,
    }


# --- Test Cases (Corrected SearchRequest Instantiation) ---


def test_search_keyword_only(search_test_environment):
    """Test searching with only a keyword across multiple files."""
    base_dir = search_test_environment["base_dir"]
    download_id = search_test_environment["download_id"]
    created_files = search_test_environment["created_files"]

    # FIX: Provide required fields: scan_keywords and extract_selector
    query = SearchRequest(
        download_id=download_id,
        scan_keywords=["APPLE"],  # Keyword for scanning phase
        extract_selector="body",  # Default selector if only scanning matters
        # Use '*' if searcher handles it as "no specific selector"
        # Using 'body' extracts main text content.
    )
    results = searcher.perform_search(query, base_dir)

    assert len(results) == 2
    urls_found = {item.original_url for item in results}
    assert URL_PAGE1 in urls_found
    assert URL_PAGE2 in urls_found

    expected_path1 = created_files[URL_PAGE1]
    result1 = next(item for item in results if item.original_url == URL_PAGE1)
    assert Path(result1.local_path) == expected_path1
    assert "keyword apple" in result1.content_preview.lower()
    # Match details *will* contain body content now due to extract_selector='body'
    assert result1.match_details is not None
    assert "keyword apple" in result1.match_details.lower()

    expected_path2 = created_files[URL_PAGE2]
    result2 = next(item for item in results if item.original_url == URL_PAGE2)
    assert Path(result2.local_path) == expected_path2
    assert "important apple info" in result2.content_preview.lower()
    assert "important apple info" in result2.match_details.lower()


def test_search_keyword_and_selector(search_test_environment):
    """Test searching with a keyword and a CSS selector."""
    base_dir = search_test_environment["base_dir"]
    download_id = search_test_environment["download_id"]
    created_files = search_test_environment["created_files"]

    # FIX: Provide required fields
    query = SearchRequest(
        download_id=download_id, scan_keywords=["BANANA"], extract_selector="p"
    )
    results = searcher.perform_search(query, base_dir)

    assert len(results) == 1
    result = results[0]
    assert result.original_url == URL_PAGE2
    expected_path = created_files[URL_PAGE2]
    assert Path(result.local_path) == expected_path
    assert "content for the second page" in result.content_preview.lower()
    assert result.match_details is not None
    assert "banana" in result.match_details.lower()
    assert "<h2>" not in result.match_details


def test_search_selector_only(search_test_environment):
    """Test searching with only a CSS selector."""
    base_dir = search_test_environment["base_dir"]
    download_id = search_test_environment["download_id"]
    created_files = search_test_environment["created_files"]

    # FIX: Provide required fields. Use placeholder for scan_keywords.
    query = SearchRequest(
        download_id=download_id,
        scan_keywords=["a"],  # Provide non-empty list to pass validation
        # Assumes searcher logic doesn't filter too strictly on this dummy keyword
        extract_selector=".highlight",
    )
    results = searcher.perform_search(query, base_dir)

    assert len(results) == 2
    urls_found = {item.original_url for item in results}
    assert URL_PAGE2 in urls_found
    assert URL_PAGE3 in urls_found

    expected_path2 = created_files[URL_PAGE2]
    result2 = next(item for item in results if item.original_url == URL_PAGE2)
    assert Path(result2.local_path) == expected_path2
    assert "Important APPLE Info" in result2.match_details

    expected_path3 = created_files[URL_PAGE3]
    result3 = next(item for item in results if item.original_url == URL_PAGE3)
    assert Path(result3.local_path) == expected_path3
    assert "More ORANGE Info" in result3.match_details


def test_search_no_matches(search_test_environment):
    """Test a search query that should yield no results."""
    base_dir = search_test_environment["base_dir"]
    download_id = search_test_environment["download_id"]

    # FIX: Provide required fields
    query = SearchRequest(
        download_id=download_id,
        scan_keywords=["nonexistentkeyword"],
        extract_selector=".nonexistentclass",
    )
    results = searcher.perform_search(query, base_dir)
    assert len(results) == 0


def test_search_keyword_in_specific_element(search_test_environment):
    """
    Test finding a keyword only when it appears within the specific element
    matched by the selector, using extract_keywords for filtering.
    """
    base_dir = search_test_environment["base_dir"]
    download_id = search_test_environment["download_id"]

    # --- Scenario 1: Find 'welcome' specifically in H1 ---
    # Scan keywords ensure the file is considered.
    # Extract selector targets H1.
    # Extract keywords filter the extracted H1 content.
    query_h1 = SearchRequest(
        download_id=download_id,
        scan_keywords=["welcome"],  # File 1 contains 'welcome' somewhere
        extract_selector="h1",
        extract_keywords=["welcome"],  # Filter extracted H1 text for 'welcome'
    )
    results_h1 = searcher.perform_search(query_h1, base_dir)
    assert len(results_h1) == 1, "Expected to find 'welcome' within H1"
    assert results_h1[0].original_url == URL_PAGE1
    # Check that the extracted detail actually *is* the H1 content containing welcome
    assert results_h1[0].match_details.lower() == "welcome"
    assert results_h1[0].selector_matched == "h1"

    # --- Scenario 2: Try to find 'welcome' specifically in P ---
    # Scan keywords ensure the file is considered.
    # Extract selector targets P.
    # Extract keywords filter the extracted P content (this filter should fail).
    query_p = SearchRequest(
        download_id=download_id,
        scan_keywords=["welcome"],  # File 1 contains 'welcome' somewhere
        extract_selector="p",
        extract_keywords=["welcome"],  # Filter extracted P text for 'welcome'
    )
    results_p = searcher.perform_search(query_p, base_dir)
    # The <p> tag in File 1 is extracted, but it doesn't contain "welcome",
    # so the extract_keywords filter removes it. Expect 0 results.
    assert len(results_p) == 0, "Expected NOT to find 'welcome' within P tags"

def test_search_non_html_skipped(search_test_environment):
    """Verify that non-HTML files are skipped during search by default."""
    base_dir = search_test_environment["base_dir"]
    download_id = search_test_environment["download_id"]

    # FIX: Provide required fields
    query = SearchRequest(
        download_id=download_id,
        scan_keywords=["color: red"],
        extract_selector="body",  # Provide default selector
    )
    results = searcher.perform_search(query, base_dir)
    assert len(results) == 0

    # FIX: Provide required fields
    query_apple = SearchRequest(
        download_id=download_id,
        scan_keywords=["APPLE"],
        extract_selector="body",  # Provide default selector
    )
    results_apple = searcher.perform_search(query_apple, base_dir)
    assert len(results_apple) == 2
    assert all(item.original_url != URL_CSS for item in results_apple)


def test_search_invalid_download_id(search_test_environment):
    """Test handling of an invalid/non-existent download ID."""
    base_dir = search_test_environment["base_dir"]
    invalid_download_id = "nonexistent_id"
    # FIX: Provide required fields
    query = SearchRequest(
        download_id=invalid_download_id,
        scan_keywords=["test"],
        extract_selector="body",  # Provide default selector
    )
    results = searcher.perform_search(query, base_dir)
    assert len(results) == 0


def test_search_malformed_index_entry(search_test_environment):
    """Test handling of a malformed line in the index file."""
    index_file = search_test_environment["index_file"]
    base_dir = search_test_environment["base_dir"]
    download_id = search_test_environment["download_id"]

    with open(index_file, "a", encoding="utf-8") as f:
        f.write("this is not json\n")

    # FIX: Provide required fields
    query = SearchRequest(
        download_id=download_id,
        scan_keywords=["APPLE"],
        extract_selector="body",  # Provide default selector
    )
    results = searcher.perform_search(query, base_dir)
    assert len(results) == 2


def test_search_missing_content_file(search_test_environment):
    """Test handling when an indexed file is missing from content dir."""
    base_dir = search_test_environment["base_dir"]
    download_id = search_test_environment["download_id"]
    created_files = search_test_environment["created_files"]

    content_path_to_delete = created_files[URL_PAGE1]
    assert content_path_to_delete.is_file()
    content_path_to_delete.unlink()
    assert not content_path_to_delete.is_file()

    # FIX: Provide required fields
    query = SearchRequest(
        download_id=download_id,
        scan_keywords=["APPLE"],
        extract_selector="body",  # Provide default selector
    )
    results = searcher.perform_search(query, base_dir)
    assert len(results) == 1
    assert results[0].original_url == URL_PAGE2
